{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TVAE module.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear, Module, Parameter, ReLU, Sequential, GELU\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ctgan.data_transformer import DataTransformer\n",
    "from ctgan.synthesizers.base import BaseSynthesizer, random_state\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "class Encoder(Module):\n",
    "    \"\"\"Encoder for the modified TVAE based on SiloFuse architecture.\"\"\"\n",
    "    def __init__(self, data_dim, embedding_dim=32, hidden_dim=1024):  # Embedding and hidden dim set to 32 and 1024 respectively.\n",
    "        super(Encoder, self).__init__()\n",
    "        print(\"Dimensions\", data_dim, embedding_dim, hidden_dim)\n",
    "        # Three linear layers with GELU activation\n",
    "        self.seq = Sequential(\n",
    "            Linear(data_dim, hidden_dim),\n",
    "            GELU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            GELU(),\n",
    "            Linear(hidden_dim, embedding_dim),\n",
    "            GELU()  # GELU activation for the last layer as well\n",
    "        )\n",
    "\n",
    "        # Latent mean and log variance (still assuming Gaussian latent space)\n",
    "        self.fc1 = Linear(hidden_dim, embedding_dim)\n",
    "        self.fc2 = Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Encode the passed `input_`.\"\"\"\n",
    "        feature = self.seq(input_)\n",
    "        mu = self.fc1(feature)\n",
    "        logvar = self.fc2(feature)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu, std, logvar\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"Decoder for the modified TVAE based on SiloFuse architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=32, hidden_dim=1024, data_dim=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Three linear layers with GELU activation\n",
    "        self.seq = Sequential(\n",
    "            Linear(embedding_dim, hidden_dim),\n",
    "            GELU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            GELU(),\n",
    "            Linear(hidden_dim, data_dim)  # Output layer with no activation as it is handled later\n",
    "        )\n",
    "        self.sigma = Parameter(torch.ones(data_dim) * 0.1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Decode the passed `input_`.\"\"\"\n",
    "        return self.seq(input_), self.sigma\n",
    "\n",
    "\n",
    "def _loss_function(recon_x, x, sigmas, mu, logvar, output_info, factor):\n",
    "    st = 0\n",
    "    loss = []\n",
    "    for column_info in output_info:\n",
    "        for span_info in column_info:\n",
    "            if span_info.activation_fn != 'softmax':\n",
    "                ed = st + span_info.dim\n",
    "                std = sigmas[st]\n",
    "                eq = x[:, st] - torch.tanh(recon_x[:, st])\n",
    "                loss.append((eq**2 / 2 / (std**2)).sum())\n",
    "                loss.append(torch.log(std) * x.size()[0])\n",
    "                st = ed\n",
    "\n",
    "            else:\n",
    "                ed = st + span_info.dim\n",
    "                loss.append(\n",
    "                    cross_entropy(\n",
    "                        recon_x[:, st:ed], torch.argmax(x[:, st:ed], dim=-1), reduction='sum'\n",
    "                    )\n",
    "                )\n",
    "                st = ed\n",
    "\n",
    "    assert st == recon_x.size()[1]\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "    return sum(loss) * factor / x.size()[0], KLD / x.size()[0]\n",
    "\n",
    "\n",
    "class TVAE(BaseSynthesizer):\n",
    "    \"\"\"TVAE.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        compress_dims=128, #(128, 128),\n",
    "        decompress_dims=128, #(128, 128),\n",
    "        l2scale=1e-5,\n",
    "        batch_size=500,\n",
    "        epochs=300,\n",
    "        loss_factor=2,\n",
    "        cuda=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.compress_dims = compress_dims\n",
    "        self.decompress_dims = decompress_dims\n",
    "\n",
    "        self.l2scale = l2scale\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_factor = loss_factor\n",
    "        self.epochs = epochs\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
    "        self.verbose = verbose\n",
    "        self.encoder = None\n",
    "\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "    @random_state\n",
    "    def fit(self, train_data, discrete_columns=()):\n",
    "        \"\"\"Fit the TVAE Synthesizer models to the training data.\n",
    "\n",
    "        Args:\n",
    "            train_data (numpy.ndarray or pandas.DataFrame):\n",
    "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
    "            discrete_columns (list-like):\n",
    "                List of discrete columns to be used to generate the Conditional\n",
    "                Vector. If ``train_data`` is a Numpy array, this list should\n",
    "                contain the integer indices of the columns. Otherwise, if it is\n",
    "                a ``pandas.DataFrame``, this list should contain the column names.\n",
    "        \"\"\"\n",
    "        self.transformer = DataTransformer()\n",
    "        self.transformer.fit(train_data, discrete_columns)\n",
    "        train_data = self.transformer.transform(train_data)\n",
    "        dataset = TensorDataset(torch.from_numpy(train_data.astype('float32')).to(self._device))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        data_dim = self.transformer.output_dimensions\n",
    "        print(data_dim, self.compress_dims, self.embedding_dim)\n",
    "        self.encoder = Encoder(data_dim, self.compress_dims, self.embedding_dim) #.to(self._device)\n",
    "        self.decoder = Decoder(self.embedding_dim, self.decompress_dims, data_dim) #.to(self._device)\n",
    "        optimizerAE = Adam(\n",
    "            list(self.encoder.parameters()) + list(self.decoder.parameters()), weight_decay=self.l2scale\n",
    "        )\n",
    "\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Batch', 'Loss'])\n",
    "        iterator = tqdm(range(self.epochs), disable=(not self.verbose))\n",
    "        if self.verbose:\n",
    "            iterator_description = 'Loss: {loss:.3f}'\n",
    "            iterator.set_description(iterator_description.format(loss=0))\n",
    "\n",
    "        for i in iterator:\n",
    "            loss_values = []\n",
    "            batch = []\n",
    "            for id_, data in enumerate(loader):\n",
    "                optimizerAE.zero_grad()\n",
    "                real = data[0].to(self._device)\n",
    "                mu, std, logvar = self.encoder(real)\n",
    "                eps = torch.randn_like(std)\n",
    "                emb = eps * std + mu\n",
    "                rec, sigmas = self.decoder(emb)\n",
    "                loss_1, loss_2 = _loss_function(\n",
    "                    rec,\n",
    "                    real,\n",
    "                    sigmas,\n",
    "                    mu,\n",
    "                    logvar,\n",
    "                    self.transformer.output_info_list,\n",
    "                    self.loss_factor,\n",
    "                )\n",
    "                loss = loss_1 + loss_2\n",
    "                loss.backward()\n",
    "                optimizerAE.step()\n",
    "                self.decoder.sigma.data.clamp_(0.01, 1.0)\n",
    "\n",
    "                batch.append(id_)\n",
    "                loss_values.append(loss.detach().cpu().item())\n",
    "\n",
    "            epoch_loss_df = pd.DataFrame({\n",
    "                'Epoch': [i] * len(batch),\n",
    "                'Batch': batch,\n",
    "                'Loss': loss_values,\n",
    "            })\n",
    "            if not self.loss_values.empty:\n",
    "                self.loss_values = pd.concat([self.loss_values, epoch_loss_df]).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "            else:\n",
    "                self.loss_values = epoch_loss_df\n",
    "\n",
    "            if self.verbose:\n",
    "                iterator.set_description(\n",
    "                    iterator_description.format(loss=loss.detach().cpu().item())\n",
    "                )\n",
    "\n",
    "    @random_state\n",
    "    def sample(self, samples):\n",
    "        \"\"\"Sample data similar to the training data.\n",
    "\n",
    "        Args:\n",
    "            samples (int):\n",
    "                Number of rows to sample.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray or pandas.DataFrame\n",
    "        \"\"\"\n",
    "        self.decoder.eval()\n",
    "\n",
    "        steps = samples // self.batch_size + 1\n",
    "        data = []\n",
    "        for _ in range(steps):\n",
    "            mean = torch.zeros(self.batch_size, self.embedding_dim)\n",
    "            std = mean + 1\n",
    "            noise = torch.normal(mean=mean, std=std).to(self._device)\n",
    "            fake, sigmas = self.decoder(noise)\n",
    "            fake = torch.tanh(fake)\n",
    "            data.append(fake.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:samples]\n",
    "        return self.transformer.inverse_transform(data, sigmas.detach().cpu().numpy())\n",
    "\n",
    "    def generate_latents(self, data):\n",
    "        \"\"\"Generate latent vectors from the input data using the trained encoder.\n",
    "\n",
    "        Args:\n",
    "            data (numpy.ndarray or pandas.DataFrame):\n",
    "                Input data to encode. Must match the format used for training.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Latent vectors generated by the encoder.\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        data = self.transformer.transform(data)\n",
    "        data_tensor = torch.from_numpy(data.astype('float32')).to(self._device)\n",
    "        with torch.no_grad():\n",
    "            mu, _, _ = self.encoder(data_tensor)  # Use mu as the latent vector\n",
    "        return mu.cpu().numpy()\n",
    "\n",
    "    def set_device(self, device):\n",
    "        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n",
    "        self._device = device\n",
    "        self.decoder.to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_column_indices(metadata_dict):\n",
    "    categorical_indices = []\n",
    "    columns = metadata_dict.get('columns', {})\n",
    "    column_names = list(columns.keys())[:-1]  # Exclude the last key\n",
    "    for index, column_name in enumerate(column_names):\n",
    "        column_data = columns[column_name]\n",
    "        if column_data.get('sdtype') == 'categorical':\n",
    "            categorical_indices.append(index)\n",
    "    return categorical_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_latent(model, source=\"../data/raw/bank.csv\", path=\"../data/processed/bank,.csv\"):\n",
    "    DATA_PATH = source\n",
    "    df = pd.read_csv(DATA_PATH, sep=\",\")\n",
    "    actual_data = df.iloc[:, :-1]\n",
    "    outcomes = df.iloc[:, -1]\n",
    "\n",
    "    latents = []\n",
    "    metadata = SingleTableMetadata()\n",
    "    meta = metadata.detect_from_csv(source)\n",
    "\n",
    "    discrete_columns = categorical_column_indices(metadata.to_dict())\n",
    "    print(discrete_columns)\n",
    "    model.fit(actual_data, discrete_columns)\n",
    "    latents = model.generate_latents(actual_data)\n",
    "    # unbatched_latent = torch.cat(latents, dim=0)\n",
    "\n",
    "    latents_df = pd.DataFrame(latents) #(unbatched_latent)\n",
    "    outcomes_df = pd.DataFrame(outcomes)\n",
    "    # Save DataFrame to a CSV file\n",
    "    data_with_outcomes = pd.concat([latents_df, outcomes_df], axis=1)\n",
    "\n",
    "    data_with_outcomes.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9, 10, 11, 12]\n",
      "90 128 128\n",
      "Dimensions 90 128 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: -53.439: 100%|██████████| 300/300 [31:30<00:00,  6.30s/it]   \n"
     ]
    }
   ],
   "source": [
    "model = TVAE()\n",
    "generate_and_save_latent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('distributed-3-9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71c2bb8a9aafac3189ddd139a8a38ed74f79c7c3756567015645529460b394b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
